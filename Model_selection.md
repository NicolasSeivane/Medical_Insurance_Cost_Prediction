[![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)](https://www.python.org/)

# ü§ñModel Selection and Evaluation

---

## üåé Language / Idioma
- [English Version](#english)
- [Versi√≥n en Espa√±ol](#espa√±ol)

---

<a name="espa√±ol"></a>
## üá™üá∏ Versi√≥n en Espa√±ol

Este documento detalla el proceso de selecci√≥n de modelos, comparando diferentes algoritmos de regresi√≥n para encontrar el mejor ajuste para predecir los cargos de seguros m√©dicos.

---

## üõ†Ô∏è Stack T√©cnico
Este an√°lisis fue realizado utilizando el lenguaje **Python** y las siguientes librer√≠as:

*   **Pandas**: Manipulaci√≥n de datos.
*   **Scikit-Learn**: Procesamiento de datos, creaci√≥n de `Pipeline` y entrenamiento de modelos.
*   **Joblib**: Guardado del modelo en formato `.pkl`.

---

## üîç Metodolog√≠a

Realizamos una b√∫squeda exhaustiva utilizando `GridSearchCV` y Validaci√≥n Cruzada (K-Fold, k=5) para asegurar m√©tricas de rendimiento robustas.

---

## üìã Grillas Utilizadas para `GridSearchCV`

| Modelo | Par√°metros / Configuraci√≥n |
| :--- | :--- |
| **Regresi√≥n Lineal** | K-folds: 5 |
| **Regresi√≥n Polin√≥mica** | K-folds: 5 \| Grado: 2, 3, 4 |
| **√Årbol de Decisi√≥n** | K-folds: 5 \| Criterion: squared_error, friedman_mse, absolute_error \| Splitter: best, random \| Max depth: None, 10, 20, 30 \| Min samples split: 2, 5, 10 \| Min samples leaf: 1, 2, 4 \| Random state: 42 |
| **Gradient Boosting** | K-folds: 5 \| N_estimators: 100, 200, 300, 500 \| Learning rate: 0.001, 0.01, 0.1, 0.2 \| Max depth: 1, 3, 5, 7 \| Min samples split: 2, 5, 10 \| Min samples leaf: 1, 2, 4 \| Random state: 42 |

---

## üìä Modelos Explorados y Resultados de Entrenamiento

| Modelo | R¬≤ Score (Test) | MAE | Tiempo (s) |
| :--- | :---: | :---: | :---: |
| **Regresi√≥n Lineal** | 0.7427 | 4227.84 | 0.0143 |
| **Regresi√≥n Polin√≥mica (Grado 2)** | 0.8189 | 2766.92 | 19.2019 |
| **√Årbol de Decisi√≥n** | 0.8577 | 1498.77 | 10.6018 |
| **Gradient Boosting** | 0.8834 | 2084.93 | 504.8198 |

---

## üèÜ Selecci√≥n del Modelo Final

> Se seleccion√≥ el **√Årbol de Decisi√≥n (Decision Tree Regressor)** con ajuste de hiperpar√°metros para el pipeline final.

### Razones de la elecci√≥n:
*   **Equilibrio Excelente**: Un alto puntaje R¬≤ (~0.85) con un Error Absoluto Medio (MAE) significativamente menor.
*   **Eficiencia**: Tiempos de entrenamiento y predicci√≥n mucho m√°s r√°pidos que Gradient Boosting.
*   **Interpretabilidad**: Caminos de decisi√≥n m√°s claros para entender incrementos de costos.

> **Mejores Par√°metros (√Årbol de Decisi√≥n):**
> *   `criterion`: 'absolute_error'
> *   `max_depth`: None
> *   `min_samples_split`: 10
> *   `splitter`: 'random'

---

## ‚öôÔ∏è Integraci√≥n del Pipeline
El modelo seleccionado se integra en un pipeline de extremo a extremo que incluye:
1. `ColumnTransformer` para escalados num√©ricos y codificaci√≥n OneHot categ√≥rica.
2. El `DecisionTreeRegressor` ya entrenado.
3. Modelo exportado como archivo `.pkl`.

---

<a name="english"></a>
## üá¨üáß English Version

This document details the model selection process, comparing different regression algorithms to find the best fit for predicting medical insurance charges.

---

## üõ†Ô∏è Technical Stack
This analysis was performed using **Python** and the following libraries:

*   **Pandas**: Data manipulation.
*   **Scikit-Learn**: Data preprocessing, `Pipeline` creation and model training.
*   **Joblib**: Model saving in `.pkl` format.

---

## üîç Methodology
We performed an extensive search using `GridSearchCV` and Cross-Validation (K-Fold, k=5) to ensure robust performance metrics.

---

## üìã Grids Used for `GridSearchCV`

| Model | Parameters / Configuration |
| :--- | :--- |
| **Linear Regression** | K-folds: 5 |
| **Polynomial Regression** | K-folds: 5 \| Degree: 2, 3, 4 |
| **Decision Tree** | K-folds: 5 \| Criterion: squared_error, friedman_mse, absolute_error \| Splitter: best, random \| Max depth: None, 10, 20, 30 \| Min samples split: 2, 5, 10 \| Min samples leaf: 1, 2, 4 \| Random state: 42 |
| **Gradient Boosting** | K-folds: 5 \| N_estimators: 100, 200, 300, 500 \| Learning rate: 0.001, 0.01, 0.1, 0.2 \| Max depth: 1, 3, 5, 7 \| Min samples split: 2, 5, 10 \| Min samples leaf: 1, 2, 4 \| Random state: 42 |

---

## üìä Models Explored and Training Results

| Model | R¬≤ Score (Test) | MAE | Training Time (s) |
| :--- | :---: | :---: | :---: |
| **Linear Regression** | 0.7427 | 4227.84 | 0.0143 |
| **Polynomial Regression (Deg 2)** | 0.8189 | 2766.92 | 19.2019 |
| **Step Tree / Decision Tree** | 0.8577 | 1498.77 | 10.6018 |
| **Gradient Boosting** | 0.8834 | 2084.93 | 504.8198 |

---

## üèÜ Final Model Selection

> [!IMPORTANT]
> The **Decision Tree Regressor** (with hyperparameter tuning) was selected for the final pipeline.

### Why this model?
*   **Excellent Balance**: A high R¬≤ score (~0.85) with a significantly lower Mean Absolute Error (MAE).
*   **Efficiency**: Much faster training and prediction times compared to Gradient Boosting.
*   **Interpretability**: Easier to understand the decision paths for cost increments.

> [!TIP]
> **Best Parameters (Decision Tree):**
> *   `criterion`: 'absolute_error'
> *   `max_depth`: None
> *   `min_samples_split`: 10
> *   `splitter`: 'random'

---

## ‚öôÔ∏è Pipeline Integration
The selected model is integrated into an end-to-end pipeline that includes:
1. `ColumnTransformer` for numerical scaling and categorical OneHot encoding.
2. The trained `DecisionTreeRegressor`.
3. Model saved in `.pkl` format.